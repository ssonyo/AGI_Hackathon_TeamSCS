from bs4 import BeautifulSoup
import re
import json
import requests
import os
import time
from tqdm import tqdm
from urllib.parse import urljoin, unquote

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# ë‹¤ìš´ë¡œë“œ í´ë” ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
download_dir = "/Users/jungyang/Desktop/YBIGTA/2025-1/í•´ì»¤í†¤/AGI_Hackathon_TeamSCS/EUM_crawling/goshi"

def get_link():
    link_for_each_eum = []
    for i in tqdm(range(1, 11)):
        link = f"https://www.eum.go.kr/web/gs/gv/gvGosiList.jsp?listSize=50&pageNo={i}&zonenm=&startdt=&enddt=&chrgorg=&selSggCd=11&select2=1100000000&select_3=&gosino=&gosichrg=&prj_nm=&prj_cat_cd=&geul_yn=Y&gihyung_yn=&silsi_yn=&mobile_yn="
        response = requests.get(link)
        soup = BeautifulSoup(response.text, 'html.parser')

        for tr in soup.find_all('tr', class_="center"):
            date = ""
            td_list = tr.find_all('td', class_="left")
            td_date = tr.find_all('td', class_="mb")
            for td in td_date:
                # 'left mb' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ td ì œì™¸
                if (td.get('class') != ['left', 'mb']) and (date == ""):
                    date = td.text

            for td in td_list:
                a_tag = td.find('a', href=True)
                if a_tag:
                    full_url = urljoin(link, a_tag['href'])
                    link_for_each_eum.append({"url": full_url, "date": date})

    return link_for_each_eum

def get_data(one_eum: dict):
    link = one_eum["url"]
    date = one_eum["date"]
    try:
        chrome_options = Options()
        chrome_options.add_experimental_option("prefs", {
            "download.default_directory": download_dir,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True,
        })
        # headless ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
        # chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")

        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        driver.get(link)
        time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        download_links = driver.find_elements(By.CSS_SELECTOR, 'a.link.font_blue04')
        print(f"[{link}] Found {len(download_links)} file(s)")

        for i, a in enumerate(download_links):
            try:
                # ë‹¤ìš´ë¡œë“œ ì „ í˜„ì¬ í´ë” ë‚´ íŒŒì¼ ëª©ë¡ í™•ì¸
                before_files = set(os.listdir(download_dir))
                
                # JavaScriptë¥¼ ì´ìš©í•˜ì—¬ hoverì™€ í´ë¦­ ì´ë²¤íŠ¸ë¥¼ ê°•ì œ ë°œìƒ
                driver.execute_script("""
                    // ë§ˆìš°ìŠ¤ ì˜¤ë²„(hover) ì´ë²¤íŠ¸ ê°•ì œ ë°œìƒ
                    var mouseOverEvent = new MouseEvent('mouseover', {
                        bubbles: true,
                        cancelable: true,
                        view: window
                    });
                    arguments[0].dispatchEvent(mouseOverEvent);
                    
                    // í´ë¦­ ì´ë²¤íŠ¸ ê°•ì œ ë°œìƒ
                    var clickEvent = new MouseEvent('click', {
                        bubbles: true,
                        cancelable: true,
                        view: window
                    });
                    arguments[0].dispatchEvent(clickEvent);
                """, a)
                print(f" âœ… Forced hover & click on file {i+1}")
                # íŒŒì¼ëª…ì—ì„œ ê´„í˜¸ì™€ ìš©ëŸ‰ ì •ë³´ë¥¼ ì œê±°í•˜ë˜, í™•ì¥ìëŠ” ìœ ì§€
                original_file_name = a.text.strip()
                if "(" in original_file_name:
                    # ë§ˆì§€ë§‰ ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ìš©ëŸ‰ ì •ë³´ë§Œ ì œê±°
                    parts = original_file_name.rsplit("(", 1)
                    if "KB" in parts[1] or "MB" in parts[1]:
                        original_file_name = parts[0].strip()
                
                # URL ë””ì½”ë”©
                original_file_name = unquote(original_file_name)

                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ëŒ€ê¸° (ì„ì‹œíŒŒì¼(.crdownload) ë°°ì œ)
                timeout = 30
                downloaded_file = None
                while timeout > 0:
                    time.sleep(1)
                    after_files = set(os.listdir(download_dir))
                    new_files = after_files - before_files
                    # ì„ì‹œíŒŒì¼(.crdownload)ì€ ì œì™¸
                    new_files = {f for f in new_files if not f.endswith(".crdownload")}
                    if new_files:
                        downloaded_file = new_files.pop()
                        break
                    timeout -= 1
                
                if downloaded_file:
                    old_path = os.path.join(download_dir, downloaded_file)
                    # new_file_name = f"{date}_{unquote(downloaded_file)}"
                    new_file_name = f"{date}_{original_file_name}"
                    new_path = os.path.join(download_dir, new_file_name)
                    print(new_path)
                    os.rename(old_path, new_path)
                    print(f" ğŸ“¦ Renamed file to: {new_file_name}")
                else:
                    print(" âš ï¸ No new file detected after download.")
                
                time.sleep(2)  # ë‹¤ìŒ ë‹¤ìš´ë¡œë“œ ì „ ì ì‹œ ëŒ€ê¸°
            except Exception as e:
                print(f" ğŸš¨ Error processing file {i+1}: {e}")

    except Exception as e:
        print(f"[Error] Failed to process link: {link}")
        print("Reason:", e)
    finally:
        driver.quit()

def main():
    # ë§í¬ ì •ë³´ë¥¼ ì²˜ìŒ ìƒì„±í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ í›„ ì‹¤í–‰í•˜ì„¸ìš”.
    # links_for_each_eum = get_link()
    # with open("EUM_crawling/links_for_each_eum.json", "w") as f:
    #     json.dump(links_for_each_eum, f)

    # ì €ì¥ëœ JSON íŒŒì¼ì—ì„œ ë§í¬ ì •ë³´ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
    json_file = "EUM_crawling/links_for_each_eum.json"
    with open(json_file, "r") as f:
        links_for_each_eum = json.load(f)

    for link in links_for_each_eum:
        get_data(link)

if __name__ == "__main__":
    main()
