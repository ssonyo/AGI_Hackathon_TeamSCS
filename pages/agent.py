# ğŸ“¦ ì „ì²´ êµ¬ì¡°: LLM ê¸°ë°˜ RAG + LangChain Agent í†µí•©í˜• + ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸

from pathlib import Path
import os
import pandas as pd
import re
import streamlit as st
from dotenv import load_dotenv
from typing import Type
from pydantic import BaseModel, Field
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings import CacheBackedEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain_upstage import ChatUpstage, UpstageEmbeddings, UpstageDocumentParseLoader
from langchain.prompts import PromptTemplate
from langchain.agents import initialize_agent, AgentExecutor
from langchain.agents.agent_types import AgentType
from langchain.callbacks.base import BaseCallbackHandler
from langchain.tools import BaseTool

load_dotenv()

st.set_page_config(page_title="êµ­í† ë¶€ë¬¸ì„œ ê¸°ë°˜ íŒŒìƒìƒí’ˆ ìƒì„± Agent", page_icon="ğŸ“„")

class ChatCallbackHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token, **kwargs):
        if "generated" not in st.session_state:
            st.session_state.generated = ""
        st.session_state.generated += token
          # ë˜ëŠ” st.chat_message("ai").write(...) ë¡œ êµì²´ ê°€ëŠ¥
llm = ChatUpstage(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    model="solar-pro",
    temperature=0.2,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

@st.cache_resource(show_spinner="ğŸ“„ ë¬¸ì„œë¥¼ ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file.read())

    docs = UpstageDocumentParseLoader(file_path, split="page").load()
    splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=600, chunk_overlap=100)
    chunks = splitter.split_documents(docs)

    embeddings = UpstageEmbeddings(
        api_key=os.getenv("UPSTAGE_API_KEY"),
        model="solar-embedding-1-large"
    )
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    vectorstore = FAISS.from_documents(chunks, cached_embeddings)
    return vectorstore.as_retriever()

@st.cache_data
def load_location_csv():
    df = pd.read_csv("pages/ë¶€ë™ì‚°_ìœ„ì¹˜ì •ë³´.csv", header=None)
    df.columns = ["ì‹œë„", "ì‹œêµ°êµ¬", "ìë©´ë™", "ìœ„ë„", "ê²½ë„"]
    return df

location_df = load_location_csv()

# ğŸ§© BaseTool ê¸°ë°˜ ì»¤ìŠ¤í…€ ë„êµ¬ í´ë˜ìŠ¤ë“¤

class ExtractDongsArgs(BaseModel):
    text: str = Field(description="í–‰ì •ë™ ì´ë¦„ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸")

class ExtractDongsTool(BaseTool):
    name: str = "ExtractDongs"
    description: str = "í…ìŠ¤íŠ¸ì—ì„œ í–‰ì •ë™ ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = ExtractDongsArgs

    def _run(self, text: str):
        return re.findall(r"\w+ë™", text)

class FindCoordsArgs(BaseModel):
    dong: str = Field(description="í–‰ì •ë™ ì´ë¦„")

class FindCoordsTool(BaseTool):
    name: str = "FindCoords"
    description: str = "í–‰ì •ë™ ì´ë¦„ìœ¼ë¡œ ìœ„ê²½ë„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = FindCoordsArgs

    def _run(self, dong: str):
        match = location_df[location_df["ìë©´ë™"].str.contains(dong)]
        if match.empty:
            return None
        return match[["ìœ„ë„", "ê²½ë„"]].values.tolist()

class GetRealEstatesArgs(BaseModel):
    coords: list = Field(description="[(ìœ„ë„, ê²½ë„)] í˜•íƒœì˜ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸")

class GetRealEstatesTool(BaseTool):
    name: str = "GetRealEstates"
    description: str = "ì£¼ì–´ì§„ ì¢Œí‘œë¡œ ì£¼ë³€ ë§¤ë¬¼ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = GetRealEstatesArgs

    def _run(self, coords: list):
        return [{"lat": lat, "lon": lon, "ë§¤ë¬¼ëª…": "OOì˜¤í”¼ìŠ¤í…”"} for lat, lon in coords]

class EstimateYieldArgs(BaseModel):
    listings: list = Field(description="ë§¤ë¬¼ ëª©ë¡")

class EstimateYieldTool(BaseTool):
    name: str = "EstimateYield"
    description: str = "ë§¤ë¬¼ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = EstimateYieldArgs

    def _run(self, listings: list):
        return f"ìˆ˜ìµë¥  ì˜ˆì¸¡ ê²°ê³¼: í‰ê·  7.2% (ì´ {len(listings)}ê°œ ë§¤ë¬¼ ê¸°ì¤€)"

class CreateFasaArgs(BaseModel):
    summary: str = Field(description="ìš”ì•½ ë‚´ìš©")

class CreateFasaTool(BaseTool):
    name: str = "CreateFasa"
    description: str = "ìµœì¢… íŒŒìƒìƒí’ˆ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = CreateFasaArgs

    def _run(self, summary: str):
        return f"íŒŒìƒìƒí’ˆ ìš”ì•½ ìƒì„± ì™„ë£Œ:\n{summary}"

tools = [
    ExtractDongsTool(),
    FindCoordsTool(),
    GetRealEstatesTool(),
    EstimateYieldTool(),
    CreateFasaTool(),
]

custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë¬¸ì„œ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
{context}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ë‹¹ì‹ ì€ ë¶€ë™ì‚°/í† ì§€ íˆ¬ì ì „ë¬¸ AI Agentì…ë‹ˆë‹¤.
í–‰ì •ë™ ì¶”ì¶œ â†’ ìœ„ê²½ë„ ì¡°íšŒ â†’ ë§¤ë¬¼ ì •ë³´ í™•ì¸ â†’ ìˆ˜ìµë¥  ê³„ì‚° â†’ íŒŒìƒìƒí’ˆ ìƒì„±ì˜ íë¦„ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
í•„ìš”ì‹œ ì•„ë˜ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì„¸ìš”:
{tool_names}

ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ë‹¤ìŒì„ ë”°ë¥´ì„¸ìš”:
Thought: ...\nAction: ...\nAction Input: ...\nObservation: ...\n...
Final Answer: ...
â— Final Answer ì´í›„ì—ëŠ” ì ˆëŒ€ ì•„ë¬´ ê²ƒë„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
"""
)

import re

def parse_and_display_output(text: str):
    """
    LangChain Agentì˜ ì¶œë ¥ í…ìŠ¤íŠ¸ë¥¼
    Thought / Action / Observation / Final Answer ë‹¨ìœ„ë¡œ íŒŒì‹±í•´ì„œ Streamlit UIì— ì¶œë ¥
    """
    pattern = r"(Thought:.*?)(?=(Thought:|Final Answer:|$))"
    thought_blocks = re.findall(pattern, text, re.DOTALL)

    for block, _ in thought_blocks:
        # ê° ë¸”ë¡ì—ì„œ êµ¬ì„± ìš”ì†Œ ì¶”ì¶œ
        thought_match = re.search(r"Thought:(.*)", block)
        action_match = re.search(r"Action:(.*)", block)
        action_input_match = re.search(r"Action Input:(.*)", block)
        obs_match = re.search(r"Observation:(.*)", block)

        with st.chat_message("ai"):
            if thought_match:
                st.markdown(f"ğŸ§  **Thought:** {thought_match.group(1).strip()}")
            if action_match:
                st.markdown(f"ğŸ”§ **Action:** `{action_match.group(1).strip()}`")
            if action_input_match:
                st.markdown(f"ğŸ“¥ **Action Input:** `{action_input_match.group(1).strip()}`")
            if obs_match:
                st.markdown(f"ğŸ“Š **Observation:** {obs_match.group(1).strip()}")

    # ë§ˆì§€ë§‰ Final Answer
    final_match = re.search(r"Final Answer:(.*)", text, re.DOTALL)
    if final_match:
        with st.chat_message("ai"):
            st.markdown(f"âœ¨ **Final Answer:**\n\n{final_match.group(1).strip()}")
            
def run_agent_with_query(query, retriever):
    st.session_state.generated = ""  # ì´ˆê¸°í™”

    context_docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in context_docs])

    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        handle_parsing_errors=True,
    )

    formatted_prompt = custom_prompt.format(
        context=context,
        question=query,
        tool_names='[' + ', '.join(tool.name for tool in tools) + ']'
    )

    # âœ… ì—ì´ì „íŠ¸ ì‹¤í–‰
    agent.run(formatted_prompt)

    # âœ… UI ì¶œë ¥ (ìŠ¤íŠ¸ë¦¬ë° ëë‚œ í›„ ì •ì œëœ ì¶œë ¥)
    parse_and_display_output(st.session_state.generated)



st.title("ğŸ“„ êµ­í† ë¶€ ê¸°ë°˜ íŒŒìƒìƒí’ˆ ì—ì´ì „íŠ¸")

file = st.sidebar.file_uploader("ğŸ“ ê³ ì‹œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf", "txt"])

if file:
    retriever = embed_file(file)
    if "generated" not in st.session_state:
        st.session_state.generated = ""
    
    # user_input = st.chat_input("ë¬¸ì„œì—ì„œ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    # if user_input:
    #     st.chat_message("user").write(user_input)
    #     with st.spinner("ì—ì´ì „íŠ¸ê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
    #         run_agent_with_query(user_input, retriever)
    st.sidebar.markdown("### ğŸ“Š ë¶„ì„ ë‹¨ê³„")
    
    if st.sidebar.button("1ï¸âƒ£ ê´€ë ¨ í–‰ì •ë™ ì¶”ì¶œ"):
        with st.spinner("í–‰ì •ë™ì„ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            query = "ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ í–‰ì •ë™ì„ ëª¨ë‘ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
            run_agent_with_query(query, retriever)
    
    if st.sidebar.button("2ï¸âƒ£ ìˆ˜ìµë¥  ë¶„ì„"):
        with st.spinner("ìˆ˜ìµë¥ ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            query = "ì¶”ì¶œëœ í–‰ì •ë™ë“¤ì˜ ì˜ˆìƒ ìˆ˜ìµë¥ ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
            run_agent_with_query(query, retriever)
    
    if st.sidebar.button("3ï¸âƒ£ ìœ„í—˜ë„ ë¶„ì„"):
        with st.spinner("ìœ„í—˜ë„ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            query = "í•´ë‹¹ ì§€ì—­ì˜ íˆ¬ì ìœ„í—˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."
            run_agent_with_query(query, retriever)
    
    if st.sidebar.button("4ï¸âƒ£ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"):
        with st.spinner("ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            query = "ì§€ê¸ˆê¹Œì§€ì˜ ë¶„ì„ì„ ì¢…í•©í•œ íˆ¬ì ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
            run_agent_with_query(query, retriever)

    # ë©”ì¸ ì˜ì—­ì— ì„¤ëª… ì¶”ê°€
    st.markdown("""
    ### ğŸ“Œ ì‚¬ìš© ë°©ë²•
    1. ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•˜ê³  ì‹¶ì€ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”
    2. ê° ë¶„ì„ ë‹¨ê³„ ë²„íŠ¼ì„ ìˆœì„œëŒ€ë¡œ í´ë¦­í•˜ì„¸ìš”:
        - í–‰ì •ë™ ì¶”ì¶œ: ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ í–‰ì •ë™ì„ ì°¾ìŠµë‹ˆë‹¤
        - ìˆ˜ìµë¥  ë¶„ì„: ê° ì§€ì—­ì˜ ì˜ˆìƒ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤
        - ìœ„í—˜ë„ ë¶„ì„: íˆ¬ì ìœ„í—˜ ìš”ì†Œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤
        - ì¢…í•© ë¦¬í¬íŠ¸: ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•©ë‹ˆë‹¤
    """)
