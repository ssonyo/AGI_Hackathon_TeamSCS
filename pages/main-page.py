# ğŸ“¦ ì „ì²´ êµ¬ì¡°: LLM ê¸°ë°˜ RAG + LangChain Agent í†µí•©í˜• + ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸

import json
from pathlib import Path
import os
import pandas as pd
import re
import streamlit as st
from dotenv import load_dotenv
from typing import Type
from pydantic import BaseModel, Field
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings import CacheBackedEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain_upstage import ChatUpstage, UpstageEmbeddings, UpstageDocumentParseLoader
from langchain.prompts import PromptTemplate
from langchain.agents import initialize_agent, AgentExecutor
from langchain.agents.agent_types import AgentType
from langchain.callbacks.base import BaseCallbackHandler
from langchain.tools import BaseTool
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from fpdf import FPDF

def extract_article_no(url):
    match = re.search(r'articleNo=(\d+)', url)
    if match:
        return match.group(1)
    return None

def parse_table_to_dict(table):
    """<table> íƒœê·¸ë¥¼ ë°›ì•„ dictë¡œ ë³€í™˜"""
    info = {}
    rows = table.select("tbody tr")
    for row in rows:
        cells = row.find_all(["th", "td"])
        for i in range(0, len(cells) - 1, 2):
            key = cells[i].get_text(strip=True)
            value = cells[i + 1].get_text(strip=True)
            info[key] = value
    return info

def get_property_details(property_list):
    results = []
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")  # â† ìµœì‹  í¬ë¡¬ì€ ì´ë ‡ê²Œ
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_experimental_option("detach", True)

    chrome_options.add_experimental_option("excludeSwitches",["enable-logging"])

    service = Service(executable_path=ChromeDriverManager().install())

    driver = webdriver.Chrome(service=service, options=chrome_options)
    wait = WebDriverWait(driver, 10)

    try:
    
        for article_no in property_list:
            try:
                url = f"https://land.naver.com/info/printArticleDetailInfo.naver?atclNo={article_no}&atclRletTypeCd=E03&rletTypeCd=E03"
                driver.get(url)
                time.sleep(1)

                soup = BeautifulSoup(driver.page_source, 'html.parser')

                tables = soup.find_all("table", summary=True)
                main_info = {}
                detail_info = {}

                for table in tables:
                    summary = table.get("summary", "")
                    if "ë§¤ë¬¼ì •ë³´" in summary:
                        main_info = parse_table_to_dict(table)
                        # ë§¤ë§¤ê°€ë¥¼ ìˆ«ìë¡œ ë³€í™˜
                        if "ë§¤ë§¤ê°€" in main_info:
                            price_str = main_info["ë§¤ë§¤ê°€"]
                            # "ë§Œì›" ì œê±°í•˜ê³  ìˆ«ìë§Œ ì¶”ì¶œ
                            price_num = int(re.sub(r'[^0-9]', '', price_str))
                            # ë§Œì› ë‹¨ìœ„ë¥¼ ì› ë‹¨ìœ„ë¡œ ë³€í™˜
                            main_info["ë§¤ë§¤ê°€_ì›"] = price_num * 10000
                            
                            # ëŒ€ì§€ë©´ì ì„ ìˆ«ìë¡œ ë³€í™˜
                            if "ëŒ€ì§€ë©´ì " in main_info:
                                area_str = main_info["ëŒ€ì§€ë©´ì "]
                                # "ã¡" ì œê±°í•˜ê³  ìˆ«ìë§Œ ì¶”ì¶œ
                                area_num = int(re.sub(r'[^0-9]', '', area_str))
                                main_info["ëŒ€ì§€ë©´ì _ã¡"] = area_num
                                
                                # ë‹¨ìœ„ë©´ì ë‹¹ ê°€ê²© ê³„ì‚°
                                if area_num > 0:
                                    main_info["ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©"] = main_info["ë§¤ë§¤ê°€_ì›"] // area_num
                    elif "ë§¤ë¬¼ì„¸ë¶€ì •ë³´" in summary:
                        detail_info = parse_table_to_dict(table)

                results.append({
                    "article_no": article_no,
                    "main_info": main_info,
                    "detail_info": detail_info
                })

            except Exception as e:
                print(f"{article_no} ë§¤ë¬¼ ìƒì„¸ ì¶”ì¶œ ì‹¤íŒ¨:", e)
                continue
    finally:
        driver.quit()

    return results


def get_property_list(latitude, longitude):
    st.write("ğŸ“ í¬ë¡¤ë§ ì‹œì‘:", latitude, longitude)
    """íŠ¹ì • ìœ„ë„, ê²½ë„ì˜ ë§¤ë¬¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    results = []
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")  # â† ìµœì‹  í¬ë¡¬ì€ ì´ë ‡ê²Œ
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_experimental_option("detach", True)

    chrome_options.add_experimental_option("excludeSwitches",["enable-logging"])

    service = Service(executable_path=ChromeDriverManager().install())

    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    try:
        
        
        # ì›¹ë“œë¼ì´ë²„ ì„¤ì •
        wait = WebDriverWait(driver, 10)
        
        # ë„¤ì´ë²„ ë¶€ë™ì‚° URL ìƒì„±
        url = f"https://new.land.naver.com/offices?ms={latitude},{longitude},16&a=TJ&b=A1&e=RETAIL"
        driver.get(url)
        time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        # ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        scroll_container = driver.find_element(By.CSS_SELECTOR, ".item_list.item_list--article")

        # ì´ˆê¸° ë†’ì´ ì„¤ì •
        prev_height = driver.execute_script("return arguments[0].scrollHeight", scroll_container)
        same_count = 0  # ë³€í™” ì—†ëŠ” íšŸìˆ˜

        while True:
            # ë‚´ë¶€ ì»¨í…Œì´ë„ˆë¥¼ ìŠ¤í¬ë¡¤
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scroll_container)
            time.sleep(1.5)

            new_height = driver.execute_script("return arguments[0].scrollHeight", scroll_container)

            if new_height == prev_height:
                same_count += 1
            else:
                same_count = 0

            if same_count >= 5:
                break

            prev_height = new_height

        # ë§¤ë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        items = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".item_list.item_list--article .item")))
        
        
        for item in items[:12]:  # ìµœëŒ€ 12ê°œê¹Œì§€ë§Œ ì²˜ë¦¬
            try:
                item.click()
                time.sleep(1)
                
                article_no = extract_article_no(driver.current_url)
                if article_no:
                    results.append(article_no)
                    st.write(f"ğŸ“ ë§¤ë¬¼ ì €ì¥: {article_no}")
                
            except Exception as e:
                st.write(f"ë§¤ë¬¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
                
        return results
                
    except Exception as e:
        st.write(f"í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return results
        
    finally:
        if driver:
            driver.quit()

# # ì‚¬ìš© ì˜ˆì‹œ
# latitude = 37.430218  # ì˜ˆì‹œ ìœ„ë„
# longitude = 127.002425  # ì˜ˆì‹œ ê²½ë„
# property_list = get_property_list(latitude, longitude)
# print(f"ì´ {len(property_list)}ê°œì˜ ë§¤ë¬¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

# a = get_property_details(property_list)
# for i in a:
#     print(i)


        


load_dotenv()

st.set_page_config(page_title="ë¯¸ë˜í˜• ë¶€ë™ì‚°/í† ì§€ íŒŒìƒí˜• ê¸ˆìœµìƒí’ˆ ìƒì„±ê¸°", page_icon="ğŸš©")

class ChatCallbackHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs):
        if "generated" not in st.session_state:
            st.session_state.generated = ""
        st.session_state.generated += token

llm = ChatUpstage(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    model="solar-pro",
    temperature=0.2,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

@st.cache_resource(show_spinner="ğŸ“„ ë¬¸ì„œë¥¼ ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file.read())

    docs = UpstageDocumentParseLoader(file_path, split="page").load()
    
    # âœ… chunk í¬ê¸° ì¶•ì†Œ
    splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)
    chunks = splitter.split_documents(docs)
    
    # âœ… 4000 í† í° ì´í•˜ë§Œ í•„í„°ë§
    filtered_chunks = [doc for doc in chunks if len(doc.page_content) <= 2800]

    embeddings = UpstageEmbeddings(
        api_key=os.getenv("UPSTAGE_API_KEY"),
        model="solar-embedding-1-large"
    )
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    vectorstore = FAISS.from_documents(filtered_chunks, cached_embeddings)
    return vectorstore.as_retriever()

@st.cache_data
def load_location_csv():
    df = pd.read_csv("pages/ë¶€ë™ì‚°_ìœ„ì¹˜ì •ë³´.csv", header=None)
    df.columns = ["ì‹œë„", "ì‹œêµ°êµ¬", "ìë©´ë™", "ìœ„ë„", "ê²½ë„"]
    return df

location_df = load_location_csv()

# Tool í´ë˜ìŠ¤ ì •ì˜
class ExtractDongsArgs(BaseModel):
    text: str = Field(description="í–‰ì •ë™ ì´ë¦„ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸")

class ExtractDongsTool(BaseTool):
    name: str = "ExtractDongs"
    description: str = "í…ìŠ¤íŠ¸ì—ì„œ ìë©´ë™ ì´ë¦„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = ExtractDongsArgs

    def _run(self, text: str):
        cleaned = re.sub(r'\w+(ì‹œ|êµ°|êµ¬)', '', text)
        return re.findall(r"\w+ë™", cleaned)

class EstimateYieldArgs(BaseModel):
    text: str = Field(description="ë¬¸ì„œ ë‚´ìš©ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸")

class EstimateYieldTool(BaseTool):
    name: str = "EstimateYield"
    description: str = "ë¬¸ì„œë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ìµë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
    args_schema: Type[BaseModel] = EstimateYieldArgs

    def _run(self, text: str):
        # ê°œë°œ ìƒíƒœ í”„ë¦¬ë¯¸ì—„ ê³„ìˆ˜ ì¶”ì¶œ
        premium_factors = []
        
        # ê³µì‚¬ì™„ë£Œ ê³ ì‹œ
        if "ê³µì‚¬ì™„ë£Œ" in text or "ì…ì£¼" in text or "ë¶„ì–‘" in text:
            premium_factors.append(1.4)
            
        # ì •ë¹„êµ¬ì—­ ê³ ì‹œ í™•ì •
        if "ì§€ì •ê³ ì‹œ" in text or "ê°œë°œì˜ˆì •" in text or "ì •ë¹„êµ¬ì—­" in text:
            premium_factors.append(1.2)
            
        # ê°œë°œ ë¯¸ì§€ì •
        if not premium_factors:
            premium_factors.append(1.0)
            
        # í‰ê·  í”„ë¦¬ë¯¸ì—„ ê³„ìˆ˜ ê³„ì‚°
        avg_premium = sum(premium_factors) / len(premium_factors)
        
        return f"ê°œë°œ ìƒíƒœ í”„ë¦¬ë¯¸ì—„ ê³„ìˆ˜: {avg_premium:.1f} (ì´ {len(premium_factors)}ê°œ ê¸°ì¤€)"

tools = [ExtractDongsTool(), EstimateYieldTool()]

analysis_prompt = PromptTemplate(
    input_variables=["context", "question", "tool_names"],
    template="""
ë¬¸ì„œ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
{context}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ë‹¹ì‹ ì€ ë¶€ë™ì‚° íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì¤‘ í•„ìš”í•œ ì‘ì—…ë§Œ ìˆ˜í–‰í•˜ì„¸ìš”:

1. í–‰ì •ë™ ì¶”ì¶œ
- ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ìë©´ë™ë§Œ ì¶”ì¶œ (ExtractDongsTool ì‚¬ìš©)
- ì‹œêµ°êµ¬ëŠ” ì œì™¸í•˜ê³  ìë©´ë™ë§Œ ì¶”ì¶œ

2. ìˆ˜ìµë¥  ë¶„ì„
- ê³ ì‹œë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ê³„ë°œê³„ìˆ˜ ì˜ˆì¸¡ (EstimateYieldTool ì‚¬ìš©)
- ì˜ˆì¸¡ëœ ê³„ë°œê³„ìˆ˜ë¥¼ ì¶”ì¶œ
- ê³„ë°œê³„ìˆ˜ ì˜ˆì¸¡ì˜ ê·¼ê±° ì¶”ì¶œ

ë„êµ¬ ëª©ë¡:
{tool_names}

ì¶œë ¥ í˜•ì‹:
Thought: ...
Action: ...
Action Input: ...
Observation: ...
Final Answer: ...
"""
)

report_prompt = PromptTemplate(
    input_variables=["context", "question", "tool_names", "recommendation"],
    template="""
ë¬¸ì„œ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
{context}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ì¶”ì²œ ë§¤ë¬¼:
{recommendation}

ë‹¹ì‹ ì€ ë¶€ë™ì‚°/í† ì§€í˜• íŒŒìƒìƒí’ˆ ìƒì„± ì „ë¬¸ í€ë“œë§¤ë‹ˆì € AI Agentì´ë©°, ê¸°ê´€ íˆ¬ììì—ê²Œ ì œì•ˆí•  **ì •ì œëœ ë¶€ë™ì‚°/í† ì§€í˜• íŒŒìƒìƒí’ˆ í¬íŠ¸í´ë¦¬ì˜¤**ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

ë³´ê³ ì„œëŠ” ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:

---

ğŸ“Œ **ë³´ê³ ì„œ ëª©ì **  
- ì‚¬ìš©ìì˜ íˆ¬ì ì¡°ê±´ê³¼ êµ­í† ë¶€ ê³ ì‹œ ë“± ê³µê³µë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë¶€ë™ì‚°/í† ì§€ ê¸°ë°˜ íŒŒìƒìƒí’ˆì„ ê¸°íš  
- ê¸°ê´€ íˆ¬ìì(ìì‚°ìš´ìš©ì‚¬, ì¦ê¶Œì‚¬, ë¦¬ì¸  ë“±)ì—ê²Œ ì œì•ˆ ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ ì •ì œëœ ë¦¬ì„œì¹˜ ë³´ê³ ì„œ ì‘ì„±

ğŸ“Œ **ì‘ì„± ì§€ì¹¨**  
- ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì—†ì´ ë¬¸ì¥ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±  
- ë¬¸ë‹¨/ì†Œì œëª©ì„ í™œìš©í•´ êµ¬ì¡°ì ì´ê³  ì„¤ë“ë ¥ ìˆê²Œ  
- ì •ì±…, ì‹œì¥ ë™í–¥ì´ ìƒí’ˆ ì„¤ê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì„¤ëª…  
- ìˆ˜ìµ íë¦„, ë¦¬ìŠ¤í¬ ë¶„ì„ ë° ì™„í™” ë°©ì•ˆ í¬í•¨  
- ì „ë¬¸ì ì´ì§€ë§Œ ì‹¤ìš©ì  ì–¸ì–´ ì‚¬ìš©

ğŸ“„ **ë³´ê³ ì„œ êµ¬ì„±**

1. ë³´ê³ ì„œ ì œëª©  
2. íˆ¬ì ê°œìš” ë° ì¡°ê±´ ìš”ì•½  
3. ê³µê³µë¬¸ì„œ ê¸°ë°˜ ì‹œì¥ ë¶„ì„  
4. ìˆ˜ìµ ëª¨ë¸ ë° íŒŒìƒìƒí’ˆ êµ¬ì¡°  
    - ë§¤ë¬¼ ì •ë³´ recommendation
    - íŒŒìƒìƒí’ˆ êµ¬ì¡° (ë§¤ë¬¼ ë¹„ìœ¨)
    - ìˆ˜ìµë¥  (EstimateYieldTool ì‚¬ìš©)
5. ë¦¬ìŠ¤í¬ ìš”ì¸ ë° ì™„í™” ë°©ì•ˆ  
6. ì¢…í•© í‰ê°€ ë° íˆ¬ì ì œì–¸  


1. í–‰ì •ë™ ì¶”ì¶œ
- ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ìë©´ë™ë§Œ ì¶”ì¶œ (ExtractDongsTool ì‚¬ìš©)
- ì‹œêµ°êµ¬ëŠ” ì œì™¸í•˜ê³  ìë©´ë™ë§Œ ì¶”ì¶œ

2. ìˆ˜ìµë¥  ë¶„ì„
- ê³ ì‹œë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ê³„ë°œê³„ìˆ˜ ì˜ˆì¸¡ (EstimateYieldTool ì‚¬ìš©)
- ì˜ˆì¸¡ëœ ê³„ë°œê³„ìˆ˜ë¥¼ ì¶”ì¶œ
- ê³„ë°œê³„ìˆ˜ ì˜ˆì¸¡ì˜ ê·¼ê±° ì¶”ì¶œ


ë„êµ¬ ëª©ë¡:
{tool_names}

---
Final Answer: ...


"""
)

def parse_and_display_output(text: str):
    pattern = r"(Thought:.*?)(?=(Thought:|Final Answer:|$))"
    thought_blocks = re.findall(pattern, text, re.DOTALL)

    for block, _ in thought_blocks:
        thought_match = re.search(r"Thought:(.*)", block)
        action_match = re.search(r"Action:(.*)", block)
        action_input_match = re.search(r"Action Input:(.*)", block)
        obs_match = re.search(r"Observation:(.*)", block)

        with st.chat_message("ai"):
            if thought_match:
                st.markdown(f"ğŸ§  **ë¶„ì„:** {thought_match.group(1).strip()}")
            if action_match:
                st.markdown(f"ğŸ”§ **ì‘ì—…:** `{action_match.group(1).strip()}`")
            if action_input_match:
                st.markdown(f"ğŸ“¥ **ì…ë ¥:** `{action_input_match.group(1).strip()}`")
            if obs_match:
                st.markdown(f"ğŸ“Š **ê²°ê³¼:** {obs_match.group(1).strip()}")

    final_match = re.search(r"Final Answer:(.*)", text, re.DOTALL)
    if final_match:
        with st.chat_message("ai"):
            st.markdown(f"âœ¨ **ìµœì¢… ê²°ê³¼:**\n\n{final_match.group(1).strip()}")

def run_agent_with_query(query, retriever):
    st.session_state.generated = ""
    context_docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in context_docs])

    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.OPENAI_FUNCTIONS,
        verbose=True,
        handle_parsing_errors=True,
    )

    formatted_prompt = analysis_prompt.format(
        context=context,
        question=query,
        tool_names='[' + ', '.join(tool.name for tool in tools) + ']'
    )

    agent.run(formatted_prompt)
    parse_and_display_output(st.session_state.generated)

def run_agent_with_query2(query, retriever, recommendation):
    st.session_state.generated = ""
    context_docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in context_docs])

    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.OPENAI_FUNCTIONS,
        verbose=True,
        handle_parsing_errors=True,
    )

    formatted_prompt = report_prompt.format(
        context=context,
        question=query,
        tool_names='[' + ', '.join(tool.name for tool in tools) + ']',
        recommendation=recommendation
    )

    agent.run(formatted_prompt)
    
    # ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    with st.container():
        st.markdown("---")
        st.markdown("## ğŸ“‘ íˆ¬ì ë¶„ì„ ë³´ê³ ì„œ")
        st.markdown("---")
        
        # ë³´ê³ ì„œ ë‚´ìš©ì„ ì„¹ì…˜ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
        report_sections = st.session_state.generated.split("\n\n")
        for section in report_sections:
            if section.strip():
                st.markdown(f"### {section.strip()}")
                st.markdown("---")
    
    # ì›ë³¸ ë¶„ì„ ë‚´ìš©ë„ í•¨ê»˜ í‘œì‹œ
    st.markdown(st.session_state.generated)

# ë©”ì¸ UI
st.title("êµ­í† ë¶€ ë¬¸ì„œ ê¸°ë°˜ ë¯¸ë˜í˜• í† ì§€/ë¶€ë™ì‚° íŒŒìƒìƒí’ˆ ìƒì„± ì—ì´ì „íŠ¸")
# ì‚¬ìš© ë°©ë²• ì•ˆë‚´
st.markdown("""
### ğŸ“Œ ì‚¬ìš© ë°©ë²•
1. ë¶„ì„í•  ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”
2. 'í–‰ì •ë™ ì¶”ì¶œ' ë²„íŠ¼ìœ¼ë¡œ ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ í–‰ì •ë™ì„ ì°¾ê±°ë‚˜
3. ì§ì ‘ í–‰ì •ë™ì„ ì…ë ¥í•˜ê³  'ë§¤ë¬¼ ë¶„ì„' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë§¤ë¬¼ ì •ë³´ì™€ ìˆ˜ìµë¥ ì„ í™•ì¸í•˜ì„¸ìš”
4. AI Agent ê°€ ì¶”ì²œí•˜ëŠ” ë¯¸ë˜í˜• í† ì§€/ë¶€ë™ì‚° íŒŒìƒ ê¸ˆìœµìƒí’ˆì˜ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”
5. ì•„ë˜ ë§í¬ì—ì„œ ê³ ì‹œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”. https://www.eum.go.kr/web/gs/gv/gvGosiList.jsp?listSize=10&pageNo=2&zonenm=&startdt=&enddt=&chrgorg=&selSggCd=11&select2=1100000000&select_3=&gosino=&gosichrg=&prj_nm=&prj_cat_cd=&geul_yn=Y&gihyung_yn=&silsi_yn=&mobile_yn=
""")



file = st.file_uploader("ğŸ“ ë¶„ì„í•  ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf", "txt"])

if file:
    retriever = embed_file(file)
    if "generated" not in st.session_state:
        st.session_state.generated = ""
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("1ï¸âƒ£ í–‰ì •ë™ ì¶”ì¶œ"):
            with st.spinner("í–‰ì •ë™ì„ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                query = "ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ í–‰ì •ë™ì„ ëª¨ë‘ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
                run_agent_with_query(query, retriever)
                st.toast("âœ… í–‰ì •ë™ ì¶”ì¶œ ì™„ë£Œ!")

    with col2:
        dong = st.text_input("ë¶„ì„í•  í–‰ì •ë™ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ë¯¸ì•„ë™")
        if st.button("2ï¸âƒ£ ë§¤ë¬¼ ë¶„ì„") and dong:
            with st.spinner(f"{dong} ë§¤ë¬¼ì„ ë¶„ì„ì¤‘ì…ë‹ˆë‹¤..."):
                # ìœ„ê²½ë„ ì¡°íšŒ
                match = location_df[location_df["ìë©´ë™"] == dong]
                if not match.empty:
                    lat = float(match.iloc[0]["ìœ„ë„"])
                    lon = float(match.iloc[0]["ê²½ë„"])
                    
                    # ë§¤ë¬¼ í¬ë¡¤ë§
                    property_list = get_property_list(lat, lon)
                    property_details = get_property_details(property_list)
                    # ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œ ë§¤ë¬¼ ì¶”ì¶œ
                    if property_details:
                        sorted_properties = sorted(
                            property_details,
                            key=lambda x: x["main_info"].get("ë‹¨ìœ„ë©´ì ë‹¹ê°€ê²©", 0),
                            reverse=True
                        )[:5]
                        property_details = sorted_properties
                        st.write(f"ğŸ“Š ì´ {len(property_details)}ê°œì˜ ë§¤ë¬¼ì´ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.warning("âš ï¸ í•´ë‹¹ ì§€ì—­ì—ì„œ ë§¤ë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        property_details = []
                    # ê²°ê³¼ ì €ì¥
                    st.session_state.latest_properties = property_details
                    st.session_state.current_dong = dong
        else:
            st.error("í•´ë‹¹ í–‰ì •ë™ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if st.button("3ï¸âƒ£ ìˆ˜ìµë¥  ë¶„ì„") and dong:
            with st.spinner(f"{dong} ìˆ˜ìµë¥ ì„ ë¶„ì„ì¤‘ì…ë‹ˆë‹¤..."):
            # ìˆ˜ìµë¥  ë¶„ì„
                query = f"{dong}ì˜ ê³„ë°œê³„ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ìµë¥ ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
                run_agent_with_query(query, retriever)
        st.toast("âœ… ë¶„ì„ ì™„ë£Œ!")
        

    # ë§¤ë¬¼ ì •ë³´ í‘œì‹œ
    if "latest_properties" in st.session_state:
        st.markdown(f"### ğŸ“‹ {st.session_state.current_dong} ì„ ì • ë§¤ë¬¼ ì •ë³´")
        for idx, prop in enumerate(st.session_state.latest_properties):
            with st.expander(f"ğŸ“Œ ë§¤ë¬¼ {idx+1}"):
                st.json(prop)
    if st.button("4ï¸âƒ£ íŒŒìƒìƒí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ìƒì„±"):
        property_details = st.session_state.latest_properties
        match = location_df[location_df["ìë©´ë™"] == dong]
        with st.spinner("í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
            # LLMì„ í†µí•œ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”
            query = f"""
            {dong}ì˜ ë§¤ë¬¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ í† ì§€í˜• íŒŒìƒìƒí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬ì„±í•´ì£¼ì„¸ìš”.
        
            ë§¤ë¬¼ì˜ ê°ê° ë¹„ì¤‘ì€ 27 26 25 24 23%ë¡œ ì¡ì•„ì£¼ì„¸ìš”
            íŒŒìƒìƒí’ˆ êµ¬ì¡°ëŠ” ë§¤ë¬¼ ë¹„ìœ¨ì„ ë°”íƒ•ìœ¼ë¡œ ì¡ì•„ì£¼ì„¸ìš”
            
            ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬ì„±í•´ì£¼ì„¸ìš”:
            1. ë¦¬ìŠ¤í¬ ë¶„ì‚°: ì§€ì—­ì , ì‹œì¥ì  ë¦¬ìŠ¤í¬ë¥¼ ê³ ë ¤í•œ ë¶„ì‚° íˆ¬ì
            2. ìˆ˜ìµë¥  ê·¹ëŒ€í™”: ë‹¨ìœ„ë©´ì ë‹¹ ê°€ê²©, ê°œë°œ ì ì¬ë ¥ ë“±ì„ ê³ ë ¤í•œ ìˆ˜ìµì„± ë¶„ì„
            3. ì§€ì—­ íŠ¹ì„±: ì¸í”„ë¼, êµí†µ, ìƒì—…ì‹œì„¤ ë“± ì§€ì—­ ê°œë°œ ì ì¬ë ¥
            4. íˆ¬ì ê¸°ê°„: ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° íˆ¬ì ì „ëµì— ë”°ë¥¸ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±
            5. ìœ ë™ì„±: ë§¤ë§¤ í™œì„±ë„, ê±°ë˜ëŸ‰ ë“±ì„ ê³ ë ¤í•œ ìœ ë™ì„± ë¶„ì„
            """
            st.markdown(f"### ğŸ“Š {dong} íŒŒìƒìƒí’ˆ í¬íŠ¸í´ë¦¬ì˜¤")
            
            portfolio_analysis = run_agent_with_query2(query, retriever, property_details)