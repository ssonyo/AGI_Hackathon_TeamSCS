from pathlib import Path
import os
import pandas as pd
import re
import streamlit as st
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_upstage import ChatUpstage, UpstageEmbeddings, UpstageDocumentParseLoader
from langchain.callbacks.base import BaseCallbackHandler
from langchain.globals import set_verbose

load_dotenv()

# --- ì„¤ì • ---
st.set_page_config(
    page_title="êµ­í† ë¶€ë¬¸ì„œ ê¸°ë°˜ í† ì§€ íŒŒìƒ ìƒí’ˆ ìƒì„± Agent",
    page_icon="ğŸ“„",
)

# --- ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ì •ì˜ ---
class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

# --- LLM ì„¸íŒ… ---
llm = ChatUpstage(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    model="solar-pro",
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

# --- ë¬¸ì„œ ì„ë² ë”© í•¨ìˆ˜ ---
@st.cache_resource(show_spinner="ğŸ” ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    loader = UpstageDocumentParseLoader(file_path, split="page")
    docs = loader.load()
    embeddings = UpstageEmbeddings(
        api_key=os.getenv("UPSTAGE_API_KEY"),
        model="solar-embedding-1-large"
    )
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

# --- ì¢Œí‘œ CSV ë¶ˆëŸ¬ì˜¤ê¸° ---
@st.cache_data
def load_location_csv():
    df = pd.read_csv("pages/ë¶€ë™ì‚°_ìœ„ì¹˜ì •ë³´.csv", encoding="utf-8", header=None)
    df.columns = ["ì‹œë„", "ì‹œêµ°êµ¬", "ìë©´ë™", "ìœ„ë„", "ê²½ë„"]
    return df

location_df = load_location_csv()

# --- í–‰ì •ë™ ì´ë¦„ìœ¼ë¡œ ìœ„ê²½ë„ ì°¾ê¸° ---
def find_coordinates(í–‰ì •ë™ì´ë¦„: str):
    match = location_df[location_df["ìë©´ë™"].str.contains(í–‰ì •ë™ì´ë¦„)]
    if match.empty:
        return None
    return match[["ìœ„ë„", "ê²½ë„"]].values.tolist()


# --- í–‰ì •ë™ ì´ë¦„ë§Œ ë½‘ê¸° ---
def extract_administrative_districts(text):
    return re.findall(r"\w+ë™", text)

# --- ì‘ë‹µì—ì„œ ì¢Œí‘œë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜ ---
def visualize_coords_from_response(response_text):
    í–‰ì •ë™ëª©ë¡ = extract_administrative_districts(response_text)
    coords_list = []

    for name in í–‰ì •ë™ëª©ë¡:
        coords = find_coordinates(name)
        if coords:
            for lat, lon in coords:
                coords_list.append({"lat": lat, "lon": lon})
            st.markdown(f"ğŸ“ **{name}** ìœ„ì¹˜ ì‹œê°„í™”ë¨")
        else:
            st.markdown(f"â“ **{name}** ì¢Œí‘œ ì •ë³´ ì—†ìŒ")

    if coords_list:
        df_coords = pd.DataFrame(coords_list)
        df_coords.columns = ["lat", "lon"]
        st.map(df_coords)




# --- ëŒ€í™” ì„¸ì…˜ ì €ì¥ ---
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )

def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

# --- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ---
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
Answer the question using ONLY the following context. 
If you're asked about administrative districts (í–‰ì •ì§€ì—­), extract them precisely from the text. 
If you don't know the answer just say you don't know. DON'T make anything up.

Context: {context}
""",
        ),
        ("human", "{question}"),
    ]
)

# --- UI ì‹œì‘ ---
st.title("ğŸ“„ êµ­í† ë¶€ë¬¸ì„œ ê¸°ë°˜ ë¶€ë™ì‚°&í† ì§€í˜• íŒŒì‚¬ìƒí’ˆ ìƒì„± Agent")

st.markdown(
    """
ì´ ì•±ì€ êµ­í† ë¶€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–‰ì •ì§€ì—­, ê°œë°œ ì •ë³´ ë“±ì„ í™•ì¸í•˜ê³  AIí€ë“œ ë§¤ë‹ˆì €ê°€ ë¶€ë™ì‚°, í† ì§€í˜• íŒŒì‚¬ìƒí’ˆì„ ìƒì„±í•˜ëŠ” ì§ˆë¬¸í•˜ê³  ì±„íŒ… ì‹œìŠ¤í…œì…ë‹ˆë‹¤.  
í–¥í›„ ì§€ë„ ê¸°ë°˜ ë§¤ë¬¼ ì¶”ì²œ ë° íŒŒì‚¬ìƒí’ˆ í† í° ê±°ë¦¬ì†Œë¡œ í™•ì¥ ì˜ˆì •ì…ë‹ˆë‹¤.

ì‘ë™ ë¡œì§:
- í–‰ì •ì§€ì—­(ì‹œ/ë„ ì‹œ/êµ°/êµ¬ ìŒ/ë©´/ë™) í™•ì¸ ë§¤ë¬¼ ì •ë³´ í¬ë¡¤ë§ -> í–‰ì •ë™ ë°”íƒ• íˆ¬ì ìˆ˜ìµë¥  ê³„ì‚°
- í–‰ì •ë™ ë°”íƒ• íˆ¬ì ìˆ˜ìµë¥  ê³„ì‚°
- íŒŒì‚¬ìƒí’ˆ ë§Œë“¤ì–´ì¤˜ -> íŒŒì‚¬ìƒí’ˆ ìƒì„± ì‹œì‘
"""
)

with st.sidebar:
    file = st.file_uploader(
        "ğŸ“„ ê³ ì‹œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF ê¶Œì¥)",
        type=["pdf", "txt", "docx"],
    )

# --- ë©”ì¸ ë¡œì§ ---
if file:
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    retriever = embed_file(file)

    send_message("\ubb38ì„œ \ubd84ì„ \uc644ë£Œ! \uad81ê¸ˆí•œ ì ì„ \ubb3cì–´ë³´ì„¸ìš” âœ¨", "ai", save=False)
    paint_history()

    message = st.chat_input("\ubb38ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”...")
    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)
            st.markdown(response.content)
            visualize_coords_from_response(response.content)
else:
    st.session_state["messages"] = []

set_verbose(True)  # ë””ë² ê° ë³´ê¸°
